{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CNNs and word embeddings\n",
    "\n",
    "Code courtesy of [this guy](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07B%20-%20Text%20Classification%20Deep%20Learning%20CNN%20Models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup script wasn't working in class - uncomment if you get errors loading modules\n",
    "#!pip install nltk beautifulsoup4 contractions tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple text processing tools\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# data wranling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, \n",
    "                                    Flatten,\n",
    "                                    Conv1D, \n",
    "                                    MaxPooling1D, \n",
    "                                    Embedding)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                            classification_report)\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "# visualisations \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "  \n",
    "    return norm_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "X_train = reviews[:35000]\n",
    "y_train = sentiments[:35000]\n",
    "\n",
    "X_test = reviews[35000:]\n",
    "y_test = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be done with scikit-learn train-test split function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_norm = pre_process_corpus(X_train)\n",
    "X_test_norm = pre_process_corpus(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define out-of-vocabulary token\n",
    "t = Tokenizer(oov_token = '<UNK>')\n",
    "\n",
    "# fit the tokenizer on then documents\n",
    "t.fit_on_texts(X_train_norm)\n",
    "\n",
    "# set padding value\n",
    "t.word_index[\"<PAD>\"] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_seqs = t.texts_to_sequences(X_train_norm)\n",
    "X_test_seqs = t.texts_to_sequences(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size={len(t.word_index)}\")\n",
    "print(f\"Number of Documents={t.document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lens = [len(s) for s in X_train_seqs]\n",
    "test_lens = [len(s) for s in X_test_seqs]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "h1 = ax[0].hist(train_lens)\n",
    "h2 = ax[1].hist(test_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add padding to sequences\n",
    "X_train_pad = sequence.pad_sequences(X_train_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_test_pad = sequence.pad_sequences(X_test_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create one-hot encodings - Do not use!\n",
    "lb = LabelBinarizer()\n",
    "y_train_lb = lb.fit_transform(y_train)\n",
    "y_test_lb = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define paramaters for model\n",
    "# overall vocublarly size\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "# number of dimensions for embeddings\n",
    "EMBED_SIZE = 300\n",
    "# number of epochs to train for\n",
    "EPOCHS = 2\n",
    "# batch size for training\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCAB_SIZE, \n",
    "                    EMBED_SIZE, \n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "# first convolution layer and pooling\n",
    "model.add(Conv1D(filters=128, \n",
    "                        kernel_size=4, \n",
    "                        padding='same',\n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# second convolution layer and pooling\n",
    "model.add(Conv1D(filters=64, \n",
    "                        kernel_size=4, \n",
    "                        padding='same', \n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, \n",
    "                        kernel_size=4, \n",
    "                        padding='same', \n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# fully-connected classification layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "                        optimizer='adam', \n",
    "                        metrics=['accuracy'])\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train_lb,\n",
    "        epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        validation_split = 0.1,\n",
    "        verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_pad, y_test_lb, verbose=1)\n",
    "print(f\"Accuracy: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0.5 decision boundary\n",
    "predictions = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "# assign labels\n",
    "predictions = ['positive' if item == 1 else 'negative' for item in predictions]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confusion matrix and classification report\n",
    "labels = ['negative', 'positive']\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions), \n",
    "             index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
